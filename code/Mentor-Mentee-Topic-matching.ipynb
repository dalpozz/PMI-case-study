{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Mentor-Mentee Topic Matching on the Enron email dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import re, os\n",
    "import json\n",
    "\n",
    "workdir = \"/Users/Andrea/Documents/Education/ULB/Phd/Github/PMI-case-study/\"\n",
    "dirmail = str(workdir)  + \"data/enron/maildir/\"\n",
    "dircode = str(workdir)  + \"code/\"\n",
    "dirLDA = str(workdir)  + \"LDA/\"\n",
    "\n",
    "os.chdir(dircode)\n",
    "from preprocessing import *\n",
    "%reload_ext preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the \"sent\" directory of each of the 150 employees of Enron. We need to import the data and in turn, clean up the data. Info from [here](https://rforwork.info/2013/11/03/a-rather-nosy-topic-model-analysis-of-the-enron-email-corpus/) and here [here](https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html) proved to be very useful. Also see http://www.colorado.edu/ics/sites/default/files/attached-files/01-11_0.pdf \n",
    "We now build a list of strings - each string being an email (document). Each document is filtered according to some regular expressions. We also build a dictionary, namely, user_docs_dict that stores for each iteration of a user, the corresponding name and as well as a list of the filtered emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nuser_docs_list = [] # Stores email sender's name and content\\nfor user in users:\\n    sent =  str(dirmail) + str(user) + '/sent'   \\n    try: \\n        os.chdir(sent)\\n        d = []\\n        for email in os.listdir():          \\n            text = open(email,'r').read()\\n            # Regular 'clutter' from each email\\n            text = remove_unwanted_text(text)\\n            d.append(text)\\n        user_docs_list.append((user, d))\\n    except:\\n        pass\\n    \\nuser_docs_dict = dict(user_docs_list)\\n\\n# Save the user_docs_dict file as json\\nwith open('user_docs_dict.jsn','w') as f:\\n    json.dump(user_docs_dict,f)\\nf.close()\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each user we extract all the emails in their inbox\n",
    "users = [i for i in os.listdir(dirmail)]\n",
    "\n",
    "'''\n",
    "user_docs_list = [] # Stores email sender's name and content\n",
    "for user in users:\n",
    "    sent =  str(dirmail) + str(user) + '/sent'   \n",
    "    try: \n",
    "        os.chdir(sent)\n",
    "        d = []\n",
    "        for email in os.listdir():          \n",
    "            text = open(email,'r').read()\n",
    "            # Regular 'clutter' from each email\n",
    "            text = remove_unwanted_text(text)\n",
    "            d.append(text)\n",
    "        user_docs_list.append((user, d))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "user_docs_dict = dict(user_docs_list)\n",
    "\n",
    "# Save the user_docs_dict file as json\n",
    "with open(dirLDA + 'user_docs_dict.jsn','w') as f:\n",
    "    json.dump(user_docs_dict,f)\n",
    "f.close()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading the user_docs_dict file\n",
    "with open(dirLDA + 'user_docs_dict.jsn','r') as f:\n",
    "    user_docs_dict = json.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check how many user and emails are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of users: 20\n"
     ]
    }
   ],
   "source": [
    "num_users = len(user_docs_dict.keys())\n",
    "print(\"Total number of users:\", num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_emails</th>\n",
       "      <th>users</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3810.0</td>\n",
       "      <td>jones-t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3774.0</td>\n",
       "      <td>shackleton-s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1862.0</td>\n",
       "      <td>taylor-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1326.0</td>\n",
       "      <td>symes-k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1315.0</td>\n",
       "      <td>kean-s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1099.0</td>\n",
       "      <td>beck-s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>968.0</td>\n",
       "      <td>fossum-d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>880.0</td>\n",
       "      <td>delainey-d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>816.0</td>\n",
       "      <td>mcconnell-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>721.0</td>\n",
       "      <td>rodrique-r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_emails         users\n",
       "13      3810.0       jones-t\n",
       "15      3774.0  shackleton-s\n",
       "18      1862.0      taylor-m\n",
       "17      1326.0       symes-k\n",
       "8       1315.0        kean-s\n",
       "9       1099.0        beck-s\n",
       "11       968.0      fossum-d\n",
       "12       880.0    delainey-d\n",
       "16       816.0   mcconnell-m\n",
       "19       721.0    rodrique-r"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "users = list(user_docs_dict.keys())\n",
    "num_emails = np.zeros(num_users)\n",
    "for i in range(num_users):\n",
    "    u = users[i]\n",
    "    user_docs = user_docs_dict[u]\n",
    "    num_emails[i] = len(user_docs)\n",
    "    \n",
    "users_df = pd.DataFrame({'users': users, 'num_emails': num_emails})\n",
    "users_df = users_df.sort_values('num_emails', ascending=False)\n",
    "users_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABEgAAAIQCAYAAACSdlt2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nc71IkIQQQgghhBBCCGGu9/8BHfhVpFHoR+sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11cb09358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 7.5)\n",
    "\n",
    "y_pos = np.arange(num_users)\n",
    "plt.bar(y_pos, users_df.num_emails, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, users_df.users, rotation=90)\n",
    "plt.ylabel('Num emails')\n",
    "plt.title('Number of emails per users')\n",
    "\n",
    "fig.savefig('num_emails.pdf', dpi=100)\n",
    "#plt.savefig(\"num_emails.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select a subset of user to speed up the computation\n",
    "import random\n",
    "num_sample = 20\n",
    "sub_users = [list(user_docs_dict.keys())[i] for i in random.sample(range(num_users), num_sample)]\n",
    "# create a new dictionary\n",
    "user_docs_dict = { user: user_docs_dict[user] for user in sub_users }\n",
    "\n",
    "users = list(user_docs_dict.keys())\n",
    "num_users = len(user_docs_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling: extract major topics in the emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract tokens, by removing stop words and using a lemmatizer (see preprocessing.py for a more detaileds).\n",
    "While creating a new 'texts' variable that stores the filtered documents, we also edit the docs_num_dict to update the words according to the tokenize,stop word, lemmatize procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all email content into a list\n",
    "from collections import defaultdict\n",
    "\n",
    "user_token_list = []\n",
    "texts = []\n",
    "for i in range(num_users):\n",
    "    user = users[i]\n",
    "    user_docs = user_docs_dict[user]\n",
    "    tokens_list = []\n",
    "    for doc in user_docs:\n",
    "        tokens = get_token(doc)\n",
    "        texts.append(tokens)\n",
    "        tokens_list.append(tokens)\n",
    "        \n",
    "    user_token_list.append((user, tokens_list))\n",
    "\n",
    "user_token_dict = dict(user_token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts file is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(dirLDA)\n",
    "\n",
    "# Save the texts file as texts_raw (will be edited again below)\n",
    "with open('texts.jsn','w') as f:\n",
    "    json.dump(texts,f)\n",
    "f.close()\n",
    "\n",
    "'''\n",
    "os.chdir(dirLDA)\n",
    "\n",
    "# Loading the raw texts file\n",
    "with open('texts_raw.jsn','r') as f:\n",
    "    texts = json.load(f)\n",
    "f.close()\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct the document term matrix whereafter the fairly lengthy process of constructing the model takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constructing a document-term matrix\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics=10\n",
    "num_passes=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=num_passes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save both the LDA data as well as the results. We can reanalyse later. See the folder called LDAdata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(dirLDA)\n",
    "\n",
    "# Saving the dictionary\n",
    "dictionary.save('dictionary')\n",
    "\n",
    "# Saving the corpus    \n",
    "with open('corpus.jsn','w') as f:\n",
    "    json.dump(corpus,f)    \n",
    "f.close()\n",
    "\n",
    "# Saving the ldamodel\n",
    "ldamodel.save('ldamodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Load dictionary\\ndictionary = corpora.Dictionary.load('dictionary')\\nldamodel = models.LdaModel.load('ldamodel') \\n# Load corpus\\nwith open('corpus.jsn','r') as f:\\n    corpus = json.load(f)\\nf.close()\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Load dictionary\n",
    "dictionary = corpora.Dictionary.load('dictionary')\n",
    "ldamodel = models.LdaModel.load('ldamodel') \n",
    "# Load corpus\n",
    "with open('corpus.jsn','r') as f:\n",
    "    corpus = json.load(f)\n",
    "f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now print the words for each of the given topics. It must be noted, that even though considerable emphasis has been placed on the construction of the regular expressions, 'junk-text' may be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word1</th>\n",
       "      <th>Word2</th>\n",
       "      <th>Word3</th>\n",
       "      <th>Word4</th>\n",
       "      <th>Word5</th>\n",
       "      <th>Word6</th>\n",
       "      <th>Word7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>\"enron\"</td>\n",
       "      <td>\"need\"</td>\n",
       "      <td>\"let\"</td>\n",
       "      <td>\"meeting\"</td>\n",
       "      <td>\"office\"</td>\n",
       "      <td>\"group\"</td>\n",
       "      <td>\"help\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>\"night\"</td>\n",
       "      <td>\"dinner\"</td>\n",
       "      <td>\"week\"</td>\n",
       "      <td>\"plan\"</td>\n",
       "      <td>\"way\"</td>\n",
       "      <td>\"female\"</td>\n",
       "      <td>\"great\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>\"gas\"</td>\n",
       "      <td>\"enrononline\"</td>\n",
       "      <td>\"power\"</td>\n",
       "      <td>\"finance\"</td>\n",
       "      <td>\"steven\"</td>\n",
       "      <td>\"miller\"</td>\n",
       "      <td>\"peter\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>\"business\"</td>\n",
       "      <td>\"enron\"</td>\n",
       "      <td>\"message\"</td>\n",
       "      <td>\"information\"</td>\n",
       "      <td>\"mail\"</td>\n",
       "      <td>\"instrumental\"</td>\n",
       "      <td>\"calger\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic5</th>\n",
       "      <td>\"company\"</td>\n",
       "      <td>\"market\"</td>\n",
       "      <td>\"share\"</td>\n",
       "      <td>\"year\"</td>\n",
       "      <td>\"power\"</td>\n",
       "      <td>\"service\"</td>\n",
       "      <td>\"nasdaq\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic6</th>\n",
       "      <td>\"deal\"</td>\n",
       "      <td>\"price\"</td>\n",
       "      <td>\"let\"</td>\n",
       "      <td>\"change\"</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>\"term\"</td>\n",
       "      <td>\"contract\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic7</th>\n",
       "      <td>\"houston\"</td>\n",
       "      <td>\"ena\"</td>\n",
       "      <td>\"london\"</td>\n",
       "      <td>\"trading\"</td>\n",
       "      <td>\"eel\"</td>\n",
       "      <td>\"legal\"</td>\n",
       "      <td>\"corporate\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic8</th>\n",
       "      <td>\"meeting\"</td>\n",
       "      <td>\"work\"</td>\n",
       "      <td>\"january\"</td>\n",
       "      <td>\"jeff\"</td>\n",
       "      <td>\"tuesday\"</td>\n",
       "      <td>\"deal\"</td>\n",
       "      <td>\"make\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic9</th>\n",
       "      <td>\"origination\"</td>\n",
       "      <td>\"corp\"</td>\n",
       "      <td>\"enron\"</td>\n",
       "      <td>\"america\"</td>\n",
       "      <td>\"north\"</td>\n",
       "      <td>\"houston\"</td>\n",
       "      <td>\"sara\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic10</th>\n",
       "      <td>\"esa\"</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>\"prep\"</td>\n",
       "      <td>\"product\"</td>\n",
       "      <td>\"trading\"</td>\n",
       "      <td>\"counterparty\"</td>\n",
       "      <td>\"master\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Word1          Word2      Word3          Word4      Word5  \\\n",
       "Topic1         \"enron\"         \"need\"      \"let\"      \"meeting\"   \"office\"   \n",
       "Topic2         \"night\"       \"dinner\"     \"week\"         \"plan\"      \"way\"   \n",
       "Topic3           \"gas\"  \"enrononline\"    \"power\"      \"finance\"   \"steven\"   \n",
       "Topic4      \"business\"        \"enron\"  \"message\"  \"information\"     \"mail\"   \n",
       "Topic5       \"company\"       \"market\"    \"share\"         \"year\"    \"power\"   \n",
       "Topic6          \"deal\"        \"price\"      \"let\"       \"change\"    \"trade\"   \n",
       "Topic7       \"houston\"          \"ena\"   \"london\"      \"trading\"      \"eel\"   \n",
       "Topic8       \"meeting\"         \"work\"  \"january\"         \"jeff\"  \"tuesday\"   \n",
       "Topic9   \"origination\"         \"corp\"    \"enron\"      \"america\"    \"north\"   \n",
       "Topic10          \"esa\"        \"trade\"     \"prep\"      \"product\"  \"trading\"   \n",
       "\n",
       "                  Word6        Word7  \n",
       "Topic1          \"group\"       \"help\"  \n",
       "Topic2         \"female\"      \"great\"  \n",
       "Topic3         \"miller\"      \"peter\"  \n",
       "Topic4   \"instrumental\"     \"calger\"  \n",
       "Topic5        \"service\"     \"nasdaq\"  \n",
       "Topic6           \"term\"   \"contract\"  \n",
       "Topic7          \"legal\"  \"corporate\"  \n",
       "Topic8           \"deal\"       \"make\"  \n",
       "Topic9        \"houston\"       \"sara\"  \n",
       "Topic10  \"counterparty\"     \"master\"  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = 7\n",
    "topic_list = ldamodel.print_topics(num_topics, num_words)\n",
    "\n",
    "def get_word_topics(topic_num, topic_list):\n",
    "    word_list = re.sub(r'(.\\....\\*)|(\\+ .\\....\\*)', '', topic_list[topic_num][1])\n",
    "    words = [word for word in word_list.split()]\n",
    "    return(words)\n",
    "    \n",
    "Topic_words =[]\n",
    "for i in range(0, num_topics):\n",
    "    words = get_word_topics(i, topic_list)    \n",
    "    Topic_words.append(words)\n",
    "\n",
    "idx = ['Topic'+str(i+1) for i in range(num_topics)]\n",
    "labels = ['Word'+str(i+1) for i in range(num_words)]\n",
    "Topics_df = pd.DataFrame.from_records(Topic_words, columns=labels, index=idx)\n",
    "Topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Mentor's expertise on each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_likelihood_per_topic(texts, ldamodel, num_topics):\n",
    "\n",
    "    score = np.zeros(num_topics)\n",
    "    \n",
    "    for text in texts:\n",
    "        #ldamodel output : [(id1, score1), (id2, score2),... if id != 0]\n",
    "        for topic_id, topic_lik in ldamodel[dictionary.doc2bow(text)]:\n",
    "            # returns each topic and the likelihood that the query relates to that topic. \n",
    "            # Gensim defaults to only showing the top ones that meet a certain threshold (>= 0.01)\n",
    "            score[topic_id] += topic_lik\n",
    "   \n",
    "    # score is the sum of likelihood on each topic\n",
    "    # we normalize the score\n",
    "    norm_score = score/np.sum(score)\n",
    "    \n",
    "    return norm_score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>Topic5</th>\n",
       "      <th>Topic6</th>\n",
       "      <th>Topic7</th>\n",
       "      <th>Topic8</th>\n",
       "      <th>Topic9</th>\n",
       "      <th>Topic10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>martin-t</th>\n",
       "      <td>0.485999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fossum-d</th>\n",
       "      <td>0.367455</td>\n",
       "      <td>0.097427</td>\n",
       "      <td>0.028354</td>\n",
       "      <td>0.072041</td>\n",
       "      <td>0.212846</td>\n",
       "      <td>0.113095</td>\n",
       "      <td>0.020699</td>\n",
       "      <td>0.020926</td>\n",
       "      <td>0.026208</td>\n",
       "      <td>0.040949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pereira-s</th>\n",
       "      <td>0.278450</td>\n",
       "      <td>0.285785</td>\n",
       "      <td>0.047551</td>\n",
       "      <td>0.065716</td>\n",
       "      <td>0.081338</td>\n",
       "      <td>0.146619</td>\n",
       "      <td>0.010508</td>\n",
       "      <td>0.030924</td>\n",
       "      <td>0.022283</td>\n",
       "      <td>0.030825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dorland-c</th>\n",
       "      <td>0.197188</td>\n",
       "      <td>0.300089</td>\n",
       "      <td>0.034005</td>\n",
       "      <td>0.039354</td>\n",
       "      <td>0.100250</td>\n",
       "      <td>0.163302</td>\n",
       "      <td>0.036722</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.031254</td>\n",
       "      <td>0.059135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grigsby-m</th>\n",
       "      <td>0.221496</td>\n",
       "      <td>0.104256</td>\n",
       "      <td>0.044408</td>\n",
       "      <td>0.044738</td>\n",
       "      <td>0.117577</td>\n",
       "      <td>0.247917</td>\n",
       "      <td>0.049105</td>\n",
       "      <td>0.059026</td>\n",
       "      <td>0.028799</td>\n",
       "      <td>0.082678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lewis-a</th>\n",
       "      <td>0.189928</td>\n",
       "      <td>0.307176</td>\n",
       "      <td>0.034755</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.155946</td>\n",
       "      <td>0.069009</td>\n",
       "      <td>0.038744</td>\n",
       "      <td>0.027676</td>\n",
       "      <td>0.033621</td>\n",
       "      <td>0.066222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taylor-m</th>\n",
       "      <td>0.306927</td>\n",
       "      <td>0.108374</td>\n",
       "      <td>0.017401</td>\n",
       "      <td>0.082696</td>\n",
       "      <td>0.092530</td>\n",
       "      <td>0.085312</td>\n",
       "      <td>0.028928</td>\n",
       "      <td>0.018846</td>\n",
       "      <td>0.071080</td>\n",
       "      <td>0.187905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kean-s</th>\n",
       "      <td>0.387303</td>\n",
       "      <td>0.072146</td>\n",
       "      <td>0.023238</td>\n",
       "      <td>0.066162</td>\n",
       "      <td>0.244512</td>\n",
       "      <td>0.068899</td>\n",
       "      <td>0.027321</td>\n",
       "      <td>0.046586</td>\n",
       "      <td>0.038786</td>\n",
       "      <td>0.025047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keavey-p</th>\n",
       "      <td>0.191307</td>\n",
       "      <td>0.150776</td>\n",
       "      <td>0.048027</td>\n",
       "      <td>0.089539</td>\n",
       "      <td>0.081379</td>\n",
       "      <td>0.174542</td>\n",
       "      <td>0.031760</td>\n",
       "      <td>0.036291</td>\n",
       "      <td>0.057949</td>\n",
       "      <td>0.138431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mims-thurston-p</th>\n",
       "      <td>0.170589</td>\n",
       "      <td>0.428923</td>\n",
       "      <td>0.035309</td>\n",
       "      <td>0.036194</td>\n",
       "      <td>0.087639</td>\n",
       "      <td>0.085754</td>\n",
       "      <td>0.026637</td>\n",
       "      <td>0.030680</td>\n",
       "      <td>0.037794</td>\n",
       "      <td>0.060481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symes-k</th>\n",
       "      <td>0.120175</td>\n",
       "      <td>0.113375</td>\n",
       "      <td>0.029226</td>\n",
       "      <td>0.028458</td>\n",
       "      <td>0.038992</td>\n",
       "      <td>0.578447</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.030360</td>\n",
       "      <td>0.018067</td>\n",
       "      <td>0.031515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ward-k</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.793720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shively-h</th>\n",
       "      <td>0.346779</td>\n",
       "      <td>0.094998</td>\n",
       "      <td>0.037865</td>\n",
       "      <td>0.064404</td>\n",
       "      <td>0.107857</td>\n",
       "      <td>0.104639</td>\n",
       "      <td>0.034532</td>\n",
       "      <td>0.053417</td>\n",
       "      <td>0.082892</td>\n",
       "      <td>0.072618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rodrique-r</th>\n",
       "      <td>0.183877</td>\n",
       "      <td>0.348682</td>\n",
       "      <td>0.042243</td>\n",
       "      <td>0.038913</td>\n",
       "      <td>0.054445</td>\n",
       "      <td>0.130103</td>\n",
       "      <td>0.104261</td>\n",
       "      <td>0.029348</td>\n",
       "      <td>0.032522</td>\n",
       "      <td>0.035606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delainey-d</th>\n",
       "      <td>0.498238</td>\n",
       "      <td>0.051348</td>\n",
       "      <td>0.023390</td>\n",
       "      <td>0.060341</td>\n",
       "      <td>0.165280</td>\n",
       "      <td>0.056062</td>\n",
       "      <td>0.041663</td>\n",
       "      <td>0.039111</td>\n",
       "      <td>0.037110</td>\n",
       "      <td>0.027458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jones-t</th>\n",
       "      <td>0.170101</td>\n",
       "      <td>0.072577</td>\n",
       "      <td>0.019210</td>\n",
       "      <td>0.055581</td>\n",
       "      <td>0.039436</td>\n",
       "      <td>0.086811</td>\n",
       "      <td>0.020536</td>\n",
       "      <td>0.020981</td>\n",
       "      <td>0.176282</td>\n",
       "      <td>0.338484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shackleton-s</th>\n",
       "      <td>0.248061</td>\n",
       "      <td>0.044751</td>\n",
       "      <td>0.012255</td>\n",
       "      <td>0.060301</td>\n",
       "      <td>0.053068</td>\n",
       "      <td>0.093076</td>\n",
       "      <td>0.023511</td>\n",
       "      <td>0.018332</td>\n",
       "      <td>0.321789</td>\n",
       "      <td>0.124855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mcconnell-m</th>\n",
       "      <td>0.416011</td>\n",
       "      <td>0.185048</td>\n",
       "      <td>0.026123</td>\n",
       "      <td>0.068670</td>\n",
       "      <td>0.129023</td>\n",
       "      <td>0.045189</td>\n",
       "      <td>0.038185</td>\n",
       "      <td>0.024911</td>\n",
       "      <td>0.036951</td>\n",
       "      <td>0.029888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white-s</th>\n",
       "      <td>0.358914</td>\n",
       "      <td>0.085634</td>\n",
       "      <td>0.037974</td>\n",
       "      <td>0.055535</td>\n",
       "      <td>0.052707</td>\n",
       "      <td>0.137446</td>\n",
       "      <td>0.163025</td>\n",
       "      <td>0.052690</td>\n",
       "      <td>0.028038</td>\n",
       "      <td>0.028039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beck-s</th>\n",
       "      <td>0.458792</td>\n",
       "      <td>0.185068</td>\n",
       "      <td>0.018036</td>\n",
       "      <td>0.061324</td>\n",
       "      <td>0.066171</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.079612</td>\n",
       "      <td>0.027786</td>\n",
       "      <td>0.029063</td>\n",
       "      <td>0.035248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Topic1    Topic2    Topic3    Topic4    Topic5    Topic6  \\\n",
       "martin-t         0.485999  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "fossum-d         0.367455  0.097427  0.028354  0.072041  0.212846  0.113095   \n",
       "pereira-s        0.278450  0.285785  0.047551  0.065716  0.081338  0.146619   \n",
       "dorland-c        0.197188  0.300089  0.034005  0.039354  0.100250  0.163302   \n",
       "grigsby-m        0.221496  0.104256  0.044408  0.044738  0.117577  0.247917   \n",
       "lewis-a          0.189928  0.307176  0.034755  0.076923  0.155946  0.069009   \n",
       "taylor-m         0.306927  0.108374  0.017401  0.082696  0.092530  0.085312   \n",
       "kean-s           0.387303  0.072146  0.023238  0.066162  0.244512  0.068899   \n",
       "keavey-p         0.191307  0.150776  0.048027  0.089539  0.081379  0.174542   \n",
       "mims-thurston-p  0.170589  0.428923  0.035309  0.036194  0.087639  0.085754   \n",
       "symes-k          0.120175  0.113375  0.029226  0.028458  0.038992  0.578447   \n",
       "ward-k           0.000000  0.070077  0.000000  0.000000  0.062905  0.000000   \n",
       "shively-h        0.346779  0.094998  0.037865  0.064404  0.107857  0.104639   \n",
       "rodrique-r       0.183877  0.348682  0.042243  0.038913  0.054445  0.130103   \n",
       "delainey-d       0.498238  0.051348  0.023390  0.060341  0.165280  0.056062   \n",
       "jones-t          0.170101  0.072577  0.019210  0.055581  0.039436  0.086811   \n",
       "shackleton-s     0.248061  0.044751  0.012255  0.060301  0.053068  0.093076   \n",
       "mcconnell-m      0.416011  0.185048  0.026123  0.068670  0.129023  0.045189   \n",
       "white-s          0.358914  0.085634  0.037974  0.055535  0.052707  0.137446   \n",
       "beck-s           0.458792  0.185068  0.018036  0.061324  0.066171  0.038900   \n",
       "\n",
       "                   Topic7    Topic8    Topic9   Topic10  \n",
       "martin-t         0.000000  0.000000  0.514001  0.000000  \n",
       "fossum-d         0.020699  0.020926  0.026208  0.040949  \n",
       "pereira-s        0.010508  0.030924  0.022283  0.030825  \n",
       "dorland-c        0.036722  0.038700  0.031254  0.059135  \n",
       "grigsby-m        0.049105  0.059026  0.028799  0.082678  \n",
       "lewis-a          0.038744  0.027676  0.033621  0.066222  \n",
       "taylor-m         0.028928  0.018846  0.071080  0.187905  \n",
       "kean-s           0.027321  0.046586  0.038786  0.025047  \n",
       "keavey-p         0.031760  0.036291  0.057949  0.138431  \n",
       "mims-thurston-p  0.026637  0.030680  0.037794  0.060481  \n",
       "symes-k          0.011385  0.030360  0.018067  0.031515  \n",
       "ward-k           0.073297  0.000000  0.000000  0.793720  \n",
       "shively-h        0.034532  0.053417  0.082892  0.072618  \n",
       "rodrique-r       0.104261  0.029348  0.032522  0.035606  \n",
       "delainey-d       0.041663  0.039111  0.037110  0.027458  \n",
       "jones-t          0.020536  0.020981  0.176282  0.338484  \n",
       "shackleton-s     0.023511  0.018332  0.321789  0.124855  \n",
       "mcconnell-m      0.038185  0.024911  0.036951  0.029888  \n",
       "white-s          0.163025  0.052690  0.028038  0.028039  \n",
       "beck-s           0.079612  0.027786  0.029063  0.035248  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_expertise = np.zeros((num_users, num_topics))\n",
    "for i in range(num_users):\n",
    "    user = users[i]\n",
    "    token_list = user_token_dict[user]\n",
    "    user_score = text_likelihood_per_topic(token_list, ldamodel, num_topics)\n",
    "    user_expertise[i,:] = user_score\n",
    "\n",
    "user_expertise\n",
    "\n",
    "cols = ['Topic'+str(i+1) for i in range(num_topics)]\n",
    "expertise_df = pd.DataFrame.from_records(user_expertise, columns=cols, index=users)\n",
    "expertise_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that row sum is zero for each user\n",
    "np.sum(user_expertise, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10000000000000001"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#average user_expertise\n",
    "np.mean(user_expertise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic1     0.279879\n",
       "Topic2     0.155326\n",
       "Topic3     0.027969\n",
       "Topic4     0.053344\n",
       "Topic5     0.097195\n",
       "Topic6     0.121256\n",
       "Topic7     0.043022\n",
       "Topic8     0.030330\n",
       "Topic9     0.081224\n",
       "Topic10    0.110455\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_expertise_per_topic = expertise_df.mean(axis=0)\n",
    "avg_expertise_per_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mentor-Mentee matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We now have a method to see which topics are prevalent for a given user (Mentor).\n",
    "Let's simulate the topic preference of a new employee (Mentee)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11103764,  0.10143279,  0.08579807,  0.12942378,  0.07731761,\n",
       "         0.14484037,  0.0878087 ,  0.04807544,  0.05859859,  0.15566702]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.random.rand(1,num_topics)\n",
    "#normalize\n",
    "nr = r/np.sum(r)\n",
    "mentee_pref = nr\n",
    "mentee_pref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that the mentee has access to the topics and provides his/her preference on each topic.\n",
    "We can reccomend an expert (Mentors) based on the mentee's preference using a distance measure (e.g. minimizing Euclidea distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# case where no preference over different topics\n",
    "mentee_no_pref = np.ones(num_topics)/num_topics\n",
    "\n",
    "# case where only one topic of interest\n",
    "mentee_one_pref = np.zeros(num_topics)\n",
    "mentee_one_pref[random.sample(range(num_topics), 1)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['keavey-p']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dist(X, y):   \n",
    "    return np.sqrt(np.sum((X-y)**2, axis=1))\n",
    "\n",
    "d = dist(user_expertise, mentee_pref)\n",
    "\n",
    "expert_match = list(np.array(users)[d == min(d)])\n",
    "expert_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume instead the mentee selects the words he/she is interested in based on the words appearing among all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sara', 'trading', 'make', 'master', 'female', 'instrumental', 'message']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take some words randomly across different topics\n",
    "flat_list = [word for sublist in Topic_words for word in sublist]\n",
    "flat_list = [x.replace('\"', '') for x in flat_list]\n",
    "words_sample = [flat_list[i] for i in random.sample(range(len(flat_list)), num_words)]\n",
    "words_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word1</th>\n",
       "      <th>Word2</th>\n",
       "      <th>Word3</th>\n",
       "      <th>Word4</th>\n",
       "      <th>Word5</th>\n",
       "      <th>Word6</th>\n",
       "      <th>Word7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic9</th>\n",
       "      <td>0.024982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word1     Word2     Word3     Word4     Word5  Word6     Word7\n",
       "Topic1   0.000000  0.000000  0.000000  0.000000  0.000000    0.0  0.000000\n",
       "Topic2   0.000000  0.000000  0.000000  0.000000  0.011134    0.0  0.000000\n",
       "Topic3   0.000000  0.000000  0.000000  0.000000  0.000000    0.0  0.000000\n",
       "Topic4   0.000000  0.000000  0.000000  0.000000  0.000000    0.0  0.011656\n",
       "Topic5   0.000000  0.000000  0.000000  0.000000  0.000000    0.0  0.000000\n",
       "Topic6   0.000000  0.000000  0.000000  0.000000  0.000000    0.0  0.000000\n",
       "Topic7   0.000000  0.031750  0.000000  0.000000  0.000000    0.0  0.000000\n",
       "Topic8   0.000000  0.000000  0.020606  0.000000  0.000000    0.0  0.000000\n",
       "Topic9   0.024982  0.000000  0.000000  0.000000  0.000000    0.0  0.000000\n",
       "Topic10  0.000000  0.013742  0.000000  0.012598  0.000000    0.0  0.000000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's compute the relevance of each word for all topics\n",
    "\n",
    "def word_likelihood_per_topic(word, ldamodel, num_topics):\n",
    "    \n",
    "    word_likelihood = np.zeros(num_topics)\n",
    "    term_topics = ldamodel.get_term_topics(word) #Returns most likely topics for a particular word\n",
    "    if term_topics:\n",
    "        for topic_tuple in term_topics:\n",
    "            topic_id = topic_tuple[0]\n",
    "            topic_lik = topic_tuple[1] #likelihood\n",
    "            word_likelihood[topic_id] = topic_lik\n",
    "    \n",
    "    return(word_likelihood)\n",
    "\n",
    "\n",
    "word_likelihoods = np.zeros((num_topics, num_words))\n",
    "for j in range(num_words):\n",
    "    w = word_likelihood_per_topic(words_sample[j], ldamodel, num_topics)\n",
    "    word_likelihoods[:,j] = w\n",
    "    \n",
    "\n",
    "idx = ['Topic'+str(i+1) for i in range(num_topics)]\n",
    "labels = ['Word'+str(i+1) for i in range(num_words)]\n",
    "word_like_df = pd.DataFrame.from_records(word_likelihoods, columns=labels, index=idx)\n",
    "word_like_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a technique from content-based reccomendaiton systems: recommend item based on a similarity comparison between the content of the items and a users profile.\n",
    "We match the mentor by computing the dot product between then word-topic likelihood and the user-topic likelihood. \n",
    "The user corrisponding to the largest values is the one with is more likely to have more knolodge about the words queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pereira-s',\n",
       " 'dorland-c',\n",
       " 'lewis-a',\n",
       " 'mims-thurston-p',\n",
       " 'symes-k',\n",
       " 'rodrique-r',\n",
       " 'mcconnell-m']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.dot(user_expertise, word_likelihoods)\n",
    "usermax = np.argmax(W, axis=1)\n",
    "list(np.array(users)[usermax == max(usermax)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TO DO \n",
    "\n",
    "#define random matching \n",
    "\n",
    "#define best matching \n",
    "\n",
    "#define worst matching \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check K-means and PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-35839563b088>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwordvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mwordvector_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \"\"\"\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "flat_texts = [text for text in texts]\n",
    "text_sample = [flat_texts[i] for i in random.sample(range(len(flat_texts)), 1000)]\n",
    "\n",
    "wordvector = TfidfVectorizer(analyzer='word', stop_words='english', max_df=0.4, min_df=5)\n",
    "wordvector_fit = wordvector.fit_transform(text_sample)\n",
    "feature = wordvector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordvector_fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-966d09eda173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k-means++'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             n_init=1)\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordvector_fit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wordvector_fit' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "N = 4\n",
    "clf = KMeans(n_clusters=N, \n",
    "            max_iter=50, \n",
    "            init='k-means++', \n",
    "            n_init=1)\n",
    "labels = clf.fit_predict(wordvector_fit)\n",
    "\n",
    "\n",
    "wordvector_fit_2d = wordvector_fit.todense()\n",
    "pca = PCA(n_components=2).fit(wordvector_fit_2d)\n",
    "datapoint = pca.transform(wordvector_fit_2d)\n",
    "\n",
    "\n",
    "label = [\"#e05f14\", \"#e0dc14\", \"#2fe014\", \"#14d2e0\"]\n",
    "color = [label[i] for i in labels]\n",
    "plt.scatter(datapoint[:, 0], datapoint[:, 1], c=color)\n",
    "\n",
    "centroids = clf.cluster_centers_\n",
    "centroidpoint = pca.transform(centroids)\n",
    "plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=150, c='#000000')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
